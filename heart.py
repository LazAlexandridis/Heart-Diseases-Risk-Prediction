# -*- coding: utf-8 -*-
"""heart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EhF1yW76LonzRFzThLAu9kL59J4ZQ1-T
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

#mount directory
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

#load data entries
fileNameFullPath = "/content/drive/MyDrive/ML/heart.csv"
df = pd.read_csv(fileNameFullPath)

#Checks if there are NaN values
nan_report = df.isna().sum()

nan_per_report = (df.isna().mean() * 100).round(2)

combined_report = pd.concat([nan_report, nan_per_report], axis=1, keys=['NaN Count', 'NaN Percentage'])

print("Combined NaN Report:")
print(combined_report)

if combined_report.values.max()  == 0:
  print("\nGood news! There are no missing values.")
else:
  print("\nThere are missing values. You should hand them appropriately.")

column_titles = df.columns
print(column_titles)

#plot for class distribution

#second column is M for male and F for female
class_col = column_titles[1]

class_labels = {'M' : 'Male', 'F' : 'Female'}


#Count people based on their gender
class_counts = df[class_col].map(class_labels.get).value_counts()



plt.figure(figsize=(8,6))

plt.bar(class_counts.index, class_counts.values, color=['blue', 'red'])

for bar in plt.bar(class_counts.index, class_counts.values):
  height = bar.get_height()
  plt.text(bar.get_x() + bar.get_width() / 2, height, '%d' % int(height), ha='center', va='bottom')

plt.xlabel('Class')
plt.ylabel('Number of patients')
plt.title('Class Distribution (M and F)')
plt.xticks(rotation=0) #optional: rotate x-axis

'''
if class_labels:
  plt.xticks(ticks=class_counts.index, labels=class_labels.values())
'''

plt.tight_layout()
plt.show()

#Count people based on whether they have a heart disease or not
dis_col = column_titles[11]

class_labels = {0 : 'No heart disease', 1 : 'Heart disease'}


dis_counts = df[dis_col].map(class_labels.get).value_counts()


plt.figure(figsize=(8,6))

plt.bar(dis_counts.index, dis_counts.values, color=['blue', 'red'])

for bar in plt.bar(dis_counts.index, dis_counts.values):
  height = bar.get_height()
  plt.text(bar.get_x() + bar.get_width() / 2, height, '%d' % int(height), ha='center', va='bottom')

plt.xlabel('Class')
plt.ylabel('Number of patients')
plt.title('Heart Disease')
plt.xticks(rotation=0) #optional: rotate x-axis

'''
if class_labels:
  plt.xticks(ticks=class_counts.index, labels=class_labels.values())
'''

plt.tight_layout()
plt.show()

# Create a 3D scatter plot for Age, Cholesterol and MaxHR
fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection='3d')

# Create a list of colors based on the values in the specified column
colors = np.where(df[column_titles[11]] == 0, 'green', 'red')

# Scatter plot with specified colors
ax.scatter(df[column_titles[0]], df[column_titles[4]], df[column_titles[7]], c=colors, marker='o')

# Set labels
ax.set_xlabel(column_titles[0])
ax.set_ylabel(column_titles[4])
ax.set_zlabel(column_titles[7])

# Add a legend
# Create custom legend handles
from matplotlib.lines import Line2D

legend_elements = [
    Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='No Heart Disease'),
    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Heart Disease')
]

# Add the legend to the plot
ax.legend(handles=legend_elements, handletextpad=2, labelspacing=1.5, loc='upper right')

# Show plot
plt.show()

#boxplots for some features (based on heart disease)
idxs = [0,3,4,7,9]

for col_idx in idxs:
    if col_idx != 1:
      plt.figure(figsize=(8,10))

      sns.boxplot(x=column_titles[11], y=df.columns[col_idx], data=df)

      plt.xticks(ticks=[0, 1], labels=['No Heart Disease', 'Heart Disease'])

      plt.title(f"Column {col_idx + 1} - {column_titles[col_idx]}")
      plt.xlabel('Class')
      plt.ylabel('Values')

#boxplots for some features (based on gender)
cols = [2,3,4,6,7,9,10]

for col_idx in cols:
  plt.figure(figsize=(8,10))

  #Create a box plot using SEABORN
  sns.boxplot(x=column_titles[1], y=df.columns[col_idx], data=df)

  plt.title(f"Column {col_idx + 1} - {column_titles[col_idx]}")
  plt.xlabel('Class')
  plt.ylabel('Values')

  plt.xticks(ticks=[0, 1], labels=['Male', 'Female'])

  plt.show()

#Label Encoding for non-numeric columns
df['Sex'], _ = pd.factorize(df['Sex'])
df['ChestPainType'], _ = pd.factorize(df['ChestPainType'])
df['RestingECG'], _ = pd.factorize(df['RestingECG'])
df['ExerciseAngina'], _ = pd.factorize(df['ExerciseAngina'])
df['ST_Slope'], _ = pd.factorize(df['ST_Slope'])

inputData = df.iloc[:, 0:11].values

#convert_class = lambda x: 0 if x == 'M' else 1

outputData = df[column_titles[11]] #.apply(convert_class).values

class_column_title = column_titles[11]

idxs = [0,3,4,7,9]

for col_idx in idxs:
  plt.figure(figsize=(8,5))

  plt.hist(df[df[class_column_title] == 0][column_titles[col_idx]], bins=33, density=True, alpha=0.7, label='No Heart Disease', color='blue')
  plt.hist(df[df[class_column_title] == 1][column_titles[col_idx]], bins=33, density=True, alpha=0.7, label='Heart Disease', color='orange')

  plt.title(f"Column {col_idx+1} - {column_titles[col_idx]}")
  plt.xlabel('Values')
  plt.ylabel('Frequency')

  plt.legend()

  plt.show()

selected_columns = [column_titles[0], column_titles[4], column_titles[7]]  # Age, Cholesterol and MaxHR

hue_labels = {0: "No heart disease", 1: "Heart disease"}

pairplot = sns.set(font_scale=0.5)  # Set the font scale (adjust the value as needed)
pairplot = sns.pairplot(df[selected_columns + [column_titles[11]]], hue=column_titles[11], hue_order=[0,1])

# Manually update the legend labels
for t, label in zip(pairplot._legend.texts, [hue_labels[0], hue_labels[1]]):
    t.set_text(label)

# Move the legend slightly to the right
pairplot._legend.set_bbox_to_anchor((0.9, 0.5))  # (x, y) coordinates
pairplot._legend.set_loc("center left")  # Keep it aligned to the left vertically

plt.show()

plt.figure(figsize=(8, 6))

sns.countplot(data=df, x=df.columns[8], hue=df.columns[11], palette="coolwarm")

plt.title("Exercise-Induced Angina vs. Heart Disease")
plt.xlabel("Exercise Angina")
plt.ylabel("Count")
plt.legend(title="Heart Disease", labels=["No", "Yes"])

plt.xticks(ticks=[0, 1], labels=['No', 'Yes'])

plt.show()

# a lot of outliers in Cholesterol

# find the correlation between cholesterol and heartDisease
before_corr = round(df['Cholesterol'].corr(df['HeartDisease']),3)
print("Correlation:", before_corr)

np.set_printoptions(precision=4, suppress=True)
print("maximum values, per feature are: ", np.max(inputData, axis=0))
print("minimum values, per feature are: ", np.min(inputData, axis=0))

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
inputData = scaler.fit_transform(inputData)

#let's discuss about value range (after MinMax)
print("maximum values, per feature are: ", np.max(inputData, axis=0))
print("minimum values, per feature are: ", np.min(inputData, axis=0))

# Use stratified k fold with k = 4 for train/test sets
# We will take in account both the health status and gender (because we have
# imbalance on the latter)
from sklearn.model_selection import StratifiedKFold

# Extract the gender column from inputData (second column)
gender = inputData[:, 1]

# Combine health status (outputData) and gender to create stratification labels
# This results in unique labels for each combination of health status and gender
strat_labels = outputData.astype(str) + "_" + gender.astype(str)

n_splits= 4
stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42) #random state ensures that each split is the same

for train_index, test_index in stratified_kfold.split(inputData, strat_labels):
  print("Train Index:", train_index)
  print("Test Index:", test_index)

  y_train, y_test = outputData[train_index], outputData[test_index]

  # Count the number of healthy (0) and non-healthy (1) patients in each set
  train_counts = np.bincount(y_train)
  test_counts = np.bincount(y_test)

  # print results
  print("\n")
  print(f"  Train - Healthy: {train_counts[0]}, Non-Healthy: {train_counts[1]}, Ratio: {train_counts[0]/train_counts[1]:.2f}") # keep 2 decimals
  print(f"  Test  - Healthy: {test_counts[0]}, Non-Healthy: {test_counts[1]}, Ratio: {train_counts[0]/train_counts[1]:.2f}") # keep 2 decimals
  print("\n")

#save results
import os
drive_path = '/content/drive/MyDrive/ML/Results'

#Check if the folder exists, and create it if necessary
if not os.path.exists(drive_path):
  os.makedirs(drive_path)

figure_folder_path = os.path.join(drive_path, 'Figures')
if not os.path.exists(figure_folder_path):
  os.makedirs(figure_folder_path)

# initialize empty list
data = []

# create the pandas DataFrame
output = pd.DataFrame()

# create a dataframe for metrics also
metrics = pd.DataFrame()

'''
data, columns=["Model", "Set", "Balanced", "TotalSamples", \
      "Non-Healthy Samples", "TP", "TN", "FP", "FN","ROC-AUC"]
'''

#import necessary ML models and performance scores
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier #personal choice
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import classification_report

import seaborn as sns
def funcy_cf_plot(cf_matrix, class_names, fullMatrixName, directoryToSave):
  '''
  class names: list of the form ['a','b','b', etc]
  we will keep two decimals
  '''
  group_counts = ["{0:0.0f}".format(value) for value in cf_matrix.flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]

  labels = [f"{v1}\n{v2}\n" for v1,v2 in zip(group_counts, group_percentages)]

  ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')

  ax.set_xlabel('Predicted heart disease')
  ax.set_ylabel('Actual heart disease')

  # Ticket labels - must be in alphabetical order
  ax.xaxis.set_ticklabels(class_names)
  ax.yaxis.set_ticklabels(class_names)

  #save the figure
  figure_path = os.path.join(directoryToSave, fullMatrixName)
  plt.savefig(figure_path)

  plt.show()

def one_model_sim_function(modelName, clf, output, metrics, foldId, inputData, outputData,\
                           train_index, test_index,\
                           figure_folder_path):

  print('. working with classifier ', modelName)

  #train the classifier
  start_time = time.time() # Record start time
  clf.fit(inputData[train_index,:], outputData[train_index])
  end_time = time.time() # Record end time

  trainingTime = end_time - start_time
  print('. just finished with training in {:.4f} seconds'.format(trainingTime))

  #use the trained classifier to estimate the outputs

  start_time = time.time() # Record start time
  predicted_output_values_train = clf.predict(inputData[train_index,:])
  predicted_output_prob_train = clf.predict_proba(inputData[train_index, :])[:, 1] # Probability for positive class
  end_time = time.time() # Record end time

  predTimeTrain = end_time - start_time

  start_time = time.time() # Record start time
  predicted_output_values_test = clf.predict(inputData[test_index,:])
  predicted_output_prob_test = clf.predict_proba(inputData[test_index, :])[:, 1] # Probability for positive class
  end_time = time.time() # Record end time

  predTimeTest = end_time - start_time

  #confusion matrices here
  cf_matrix_train = confusion_matrix(outputData[train_index],\
                                  predicted_output_values_train )

  cf_matrix_test = confusion_matrix(outputData[test_index],\
                                predicted_output_values_test )


  #create some figures with confusion matrices
  funcy_cf_plot(cf_matrix_train, ['HeartDisease','NoHeartDisease'],\
                modelName + str(foldCounter) + "Train",\
                figure_folder_path)
  funcy_cf_plot(cf_matrix_test, ['HeartDisease','NoHeartDisease'],\
                modelName + str(foldCounter) + "Test",\
                figure_folder_path)

  print('. just finished with saving the confusion matrices.')

  # calculate and print metrics for training set
  train_accuracy = accuracy_score(outputData[train_index], predicted_output_values_train)
  train_precision = precision_score(outputData[train_index], predicted_output_values_train)
  train_recall = recall_score(outputData[train_index], predicted_output_values_train)
  train_f1 = f1_score(outputData[train_index], predicted_output_values_train)
  train_auc = roc_auc_score(outputData[train_index], predicted_output_prob_train)


  print(f"Train - Accuracy: {train_accuracy:.2f}, Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1 Score: {train_f1:.2f}, ROC-AUC: {train_auc:.2f}")


  # calculate and print metrics for test set
  test_accuracy = accuracy_score(outputData[test_index], predicted_output_values_test)
  test_precision = precision_score(outputData[test_index], predicted_output_values_test)
  test_recall = recall_score(outputData[test_index], predicted_output_values_test)
  test_f1 = f1_score(outputData[test_index], predicted_output_values_test)
  test_auc = roc_auc_score(outputData[test_index], predicted_output_prob_test)

  print(f"Test  - Accuracy: {test_accuracy:.2f}, Precision: {test_precision:.2f}, Recall: {test_recall:.2f}, F1 Score: {test_f1:.2f}, ROC-AUC: {test_auc:.2f}")


  # add line for metrics (train) to a csv

  tmp_line_to_append = {'Model': modelName, 'Set': "Train", "Fold" : foldId, "Accuracy" : train_accuracy, "Precision" : train_precision, "Recall" : train_recall, "F1" : train_f1, "ROC-AUC" : train_auc}
  metrics = pd.concat([metrics, pd.DataFrame([tmp_line_to_append])], ignore_index=True)


  # add line for metrics (test) to a csv

  tmp_line_to_append = {'Model': modelName, 'Set': "Test", "Fold" : foldId, "Accuracy" : test_accuracy, "Precision" : test_precision, "Recall" : test_recall, "F1" : test_f1, "ROC-AUC" : test_auc}
  metrics = pd.concat([metrics, pd.DataFrame([tmp_line_to_append])], ignore_index=True)



  #add line for train data

  tmp_line_to_append = {'Model': modelName, 'Set': "Train", 'Balanced' : "Balanced", 'TotalSamples': sum(outputData[train_index]==0)+sum(outputData[train_index]==1)\
                        ,'Non-Healthy Samples' : sum(outputData[train_index]==1), "TP" : cf_matrix_train[1,1], "TN" : cf_matrix_train[0,0] \
                        , "FP" : cf_matrix_train[0,1], "FN" : cf_matrix_train[1,0], "ROC-AUC" : train_auc}

  output = pd.concat([output, pd.DataFrame([tmp_line_to_append])], ignore_index=True)




  #add line for test data
  tmp_line_to_append = {'Model': modelName, 'Set': "Test", 'Balanced' : "Balanced", 'TotalSamples': sum(outputData[test_index]==0)+sum(outputData[test_index]==1)\
                        ,'Non-Healthy Samples' : sum(outputData[test_index]==1), "TP" : cf_matrix_test[1,1], "TN" : cf_matrix_test[0,0] \
                        , "FP" : cf_matrix_test[0,1], "FN" : cf_matrix_test[1,0], "ROC-AUC" : test_auc}


  output = pd.concat([output, pd.DataFrame([tmp_line_to_append])], ignore_index=True)



  print('. just finished with data append to csv file.')
  print("\n")



  return output, metrics

# Hyperparameter Optimization
# Below are some common variables (and libraries) for each architecture

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
import time

n_iter = 20 # number of iterations
scoring = 'accuracy' # metric
cv = 4 # 4-fold cross-validation
random_state = 42 # seed
n_jobs = -1 # use all CPU cores

# Start runing each classification model
# We pass the tuned arguments to each model
foldCounter = 1
start = time.time()
for train_index, test_index in\
  stratified_kfold.split(inputData, strat_labels):

  print('we are currently in fold set no: ', str(foldCounter))

  # We run Hyperparameter Optimization on each fold. Fit the data on train only!

  #1. LDA
  # Define the parameters for LinearDiscriminantAnalysis HypeOpt
  param_distributions = {
      'solver' :  ['lsqr', 'eigen'],
      'shrinkage' : [None, 'auto'] + list(np.linspace(0.0, 1.0, 10)) #svd doesnt have shrinkage
  }

  # Set up the RandomizedSearchCV with the LDA model
  random_searchLDA = RandomizedSearchCV(
      estimator = LinearDiscriminantAnalysis(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state = random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchLDA.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchLDA.best_params_)
  print("Best cross-validation accuracy:", random_searchLDA.best_score_)
  #now use the hypeopt model
  output, metrics = one_model_sim_function("LDA", LinearDiscriminantAnalysis(**random_searchLDA.best_params_), output, metrics, foldCounter, inputData, outputData,\
                           train_index, test_index,\
                          figure_folder_path)



  #2. LR
  # Define the parameters for LogisticRegression HypeOpt
  param_distributions = {
      'penalty' :  [None, 'l2'], #lbfgs supports only L2 penalty and None
      'C' : uniform(0.1, 10),
      'fit_intercept' : [True, False]
  }

  # Set up the RandomizedSearchCV with the LR model
  random_searchLR = RandomizedSearchCV(
      estimator = LogisticRegression(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state= random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchLR.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchLR.best_params_)
  print("Best cross-validation accuracy:", random_searchLR.best_score_)


  output, metrics = one_model_sim_function("LR", LogisticRegression(**random_searchLR.best_params_), output, metrics, foldCounter, inputData, outputData,\
                           train_index, test_index,\
                          figure_folder_path)


  #3. Ctree
  # Define the parameters for DecisionTree HypeOpt
  param_distributions = {
      'criterion' :  ['gini', 'entropy', 'log_loss'],
      'splitter' : ['best', 'random'],
      'max_depth' : randint(1,100),
      'min_samples_split' : randint(2,100)
  }

  # Set up the RandomizedSearchCV with the LR model
  random_searchDT = RandomizedSearchCV(
      estimator = DecisionTreeClassifier(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state = random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchDT.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchDT.best_params_)
  print("Best cross-validation accuracy:", random_searchDT.best_score_)

  output, metrics = one_model_sim_function("Ctree", DecisionTreeClassifier(**random_searchDT.best_params_), output, metrics, foldCounter, inputData, outputData,\
                           train_index, test_index,\
                          figure_folder_path)




  #4. RF
  # Define the parameters for RandomForest HypeOpt
  param_distributions = {
      'criterion' :  ['gini', 'entropy', 'log_loss'],
      'n_estimators' : randint(10,500),
      'max_depth' : randint(1,100),
      'min_samples_split' : randint(2,100)
  }

  # Set up the RandomizedSearchCV with the RF model
  random_searchRF = RandomizedSearchCV(
      estimator = RandomForestClassifier(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state = random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchRF.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchRF.best_params_)
  print("Best cross-validation accuracy:", random_searchRF.best_score_)

  output, metrics = one_model_sim_function("RF", RandomForestClassifier(**random_searchRF.best_params_), output, metrics, foldCounter, inputData, outputData,\
                           train_index, test_index,\
                          figure_folder_path)



  #5. kNN
  # Define the parameters for kNearestNeighbors HypeOpt
  param_distributions = {
      'n_neighbors' :  randint(1,10),
      'weights' : ['uniform', 'distance'],
      'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],
      'leaf_size' : randint(30,100),
      'p': randint(1,5)
  }

  # Set up the RandomizedSearchCV with the kNN model
  random_searchkNN = RandomizedSearchCV(
      estimator = KNeighborsClassifier(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state = random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchkNN.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchkNN.best_params_)
  print("Best cross-validation accuracy:", random_searchkNN.best_score_)
  output, metrics = one_model_sim_function("kNN", KNeighborsClassifier(**random_searchkNN.best_params_), output, metrics, foldCounter, inputData, outputData,\
                          train_index, test_index,\
                        figure_folder_path)


  #6. Gaussian NB
  # Define the parameters for Gaussian Naïve Bayes HypeOpt
  param_distributions = {
      'var_smoothing' : np.logspace(0, -9, num=100)
  }

  # Set up the RandomizedSearchCV with the NB model
  random_searchNB = RandomizedSearchCV(
      estimator = GaussianNB(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state = random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchNB.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchNB.best_params_)
  print("Best cross-validation accuracy:", random_searchNB.best_score_)

  output, metrics = one_model_sim_function("NB", GaussianNB(**random_searchNB.best_params_), output, metrics, foldCounter, inputData, outputData,\
                          train_index, test_index,\
                        figure_folder_path)


  #7. SVM (SVC)
  # Define the parameters for Support Vector Machines HypeOpt
  param_distributions = {
      'C' : uniform(0.1, 100),
      'kernel' : ['linear', 'rbf'],
      'gamma' : ['scale', 'auto'],
      'degree' : randint(1, 10),
      'probability' : [True] # We always need probability estimates in order to calculate ROC-AUC
  }

  # Set up the RandomizedSearchCV for SVM model HypeOpt
  random_searchSVC = RandomizedSearchCV(
      estimator = SVC(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state = random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchSVC.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchSVC.best_params_)
  print("Best cross-validation accuracy:", random_searchSVC.best_score_)

  output, metrics = one_model_sim_function("SVM", SVC(**random_searchSVC.best_params_), output, metrics, foldCounter, inputData, outputData,\
                          train_index, test_index,\
                        figure_folder_path)


  #8. Multi-layer Perceptron (MLP) Personal Choice
  # Define the parameters for Multi-Layer Perceptron HypeOpt
  param_distributions = {
      'activation' : ['identity', 'logistic', 'tanh', 'relu'],
      'solver' : ['lbfgs', 'sgd', 'adam'],
      'alpha' : uniform(0.1, 10),
      'batch_size' : randint(16, 128),
      'learning_rate' : ['constant', 'invscaling', 'adaptive'],
      'max_iter' : randint(10, 1000)
  }

  # Set up the RandomizedSearchCV with the MLP model
  random_searchMLP = RandomizedSearchCV(
      estimator = MLPClassifier(),
      param_distributions=param_distributions,
      n_iter = n_iter,
      scoring = scoring,
      cv = cv,
      random_state = random_state,
      n_jobs = n_jobs
  )

  # Timer start
  start_time = time.time()

  # Fit RandomizedSearchCV on train
  random_searchMLP.fit(inputData[train_index,:], outputData[train_index])

  # Timer stop
  end_time = time.time()

  # Calculate execution time
  execution_time = end_time - start_time
  print(f"Hyperparameter optimization took {execution_time:.2f} seconds")


  # Output the best parameters and the best score
  print("Best parameters found:", random_searchMLP.best_params_)
  print("Best cross-validation accuracy:", random_searchMLP.best_score_)

  output, metrics = one_model_sim_function("MLP", MLPClassifier(**random_searchMLP.best_params_), output, metrics, foldCounter, inputData, outputData,\
                          train_index, test_index,\
                        figure_folder_path)



  #update the fold counter
  foldCounter +=1;
stop = time.time()
print(f"Total time for all models (with HypeOpt): {stop-start:.2f} seconds")

#write results to file
output.to_csv(drive_path + '/balancedDataOutcomes.csv', index=False)
metrics.to_csv(drive_path + '/metrics.csv', index=False)

import scipy.stats as stats

# Load the CSV file
df = pd.read_csv('/content/drive/MyDrive/ML/Results/metrics.csv')

# Check the first few rows
df.head()

# Filter to only include the Test set
test_set_df = df[df['Set'] == 'Test']

# Check the first few rows
test_set_df.head()

# Residuals for Normality Check (Grouped by Model)
import statsmodels.api as sm

# Unique models
models = test_set_df['Model'].unique()

# Initialize dictionary to store Shapiro-Wilk test results
shapiro_results = {}

# Loop through each model
for model in models:
    print(f"Normality Check for {model}")

    # Filter scores for the current model
    model_scores = test_set_df[test_set_df['Model'] == model]['ROC-AUC']

    # Shapiro-Wilk Test
    shapiro_test = stats.shapiro(model_scores)
    shapiro_results[model] = shapiro_test
    print(f"  Shapiro-Wilk Test Result: {shapiro_test}")

    # Q-Q Plot
    sm.qqplot(model_scores, line='s')
    plt.title(f"Q-Q Plot of ROC-AUC score for {model}")
    plt.show()

# Print all Shapiro results for reference
print("\nShapiro-Wilk Test Results:\n")
for model, result in shapiro_results.items():
    print(f"{model}: Statistic={result.statistic}, p-value={result.pvalue}")
    if result.pvalue <= 0.05:
      print( "Unfortunately scores significantly deviate from a normal distribution\n")
    else:
      print( "Good news! scores follow a normal distribution\n")

# 2. Homogeneity of Variances (since we have normal distribution)
levene_test = stats.levene(*[test_set_df[test_set_df['Model'] == model]['ROC-AUC']
                             for model in test_set_df['Model'].unique()])
print("Levene's Test for Homogeneity of Variance:", levene_test)

# Boxplot for Variance Visualization
sns.boxplot(x='Model', y='ROC-AUC', data=test_set_df)
plt.title("Boxplot of ROC-AUC by Model")
plt.ylabel('ROC-AUC')
plt.show()

if levene_test.pvalue < 0.05:
  print("\nSignificant difference in variances detected")
else:
  print("\nGood News! no significant difference in variances detected")

# more boxplots

metrics = ['Accuracy', 'Precision', 'Recall', 'F1']

for metric in metrics:
  sns.boxplot(x='Model', y=metric, data=test_set_df)
  plt.title(f"Boxplot of {metric} by Model")
  plt.ylabel(f'{metric}')
  plt.show()

# ANOVA assumptions met (models follow a normal distribution and Levene's test show no differences in variances)
# lets proceed with ANOVA
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Group ROC-AUC values by Model
anova_groups = [test_set_df[test_set_df['Model'] == model]['ROC-AUC'].values
                for model in test_set_df['Model'].unique()]

# One-way ANOVA
anova_result = stats.f_oneway(*anova_groups)
print("ANOVA Result:")
print(f"F-statistic: {anova_result.statistic}, p-value: {anova_result.pvalue}")

# Check if the result is significant
if anova_result.pvalue <= 0.05:
    print("\nSignificant differences detected. Proceeding with post-hoc analysis...")

    # Tukey's HSD (Post-hoc test)
    tukey_result = pairwise_tukeyhsd(endog=test_set_df['ROC-AUC'],      # Dependent variable
                                     groups=test_set_df['Model'],  # Grouping variable
                                     alpha=0.05)                   # Significance level
    print(tukey_result)

    # Plot the Tukey results
    tukey_result.plot_simultaneous( figsize=(8, 6))
    plt.title("Tukey HSD Post-hoc Test Results")
    plt.show()
else:
    print("\nNo significant differences detected between models.")